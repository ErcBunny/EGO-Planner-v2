{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Grad Calculation\n",
    "\n",
    "Find grad of states w.r.t. coeffs and times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Angular Velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\frac{\\partial \\omega}{\\partial c_i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sample code automatically generated on 2023-10-31 12:54:39\n",
    "\n",
    "by www.matrixcalculus.org\n",
    "\n",
    "from input\n",
    "\n",
    "d/dc (beta2'*c*z*c'*beta1)/(beta1'*c*c'*beta1) = 1/(beta1'*c*c'*beta1)*beta2*(beta1'*c*z')+1/(beta1'*c*c'*beta1)*beta1*(beta2'*c*z)-(1/(beta1'*c*c'*beta1).^2*beta1'*c*z'*c'*beta2*beta1*(beta1'*c)+1/(beta1'*c*c'*beta1).^2*beta2'*c*z*c'*beta1*beta1*(beta1'*c))\n",
    "\n",
    "where\n",
    "\n",
    "beta1 is a vector\n",
    "beta2 is a vector\n",
    "c is a matrix\n",
    "z is a matrix\n",
    "\n",
    "The generated code is provided \"as is\" without warranty of any kind.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def fAndG(beta1, beta2, c, z):\n",
    "    assert isinstance(beta1, np.ndarray)\n",
    "    dim = beta1.shape\n",
    "    assert len(dim) == 1\n",
    "    beta1_rows = dim[0]\n",
    "    assert isinstance(beta2, np.ndarray)\n",
    "    dim = beta2.shape\n",
    "    assert len(dim) == 1\n",
    "    beta2_rows = dim[0]\n",
    "    assert isinstance(c, np.ndarray)\n",
    "    dim = c.shape\n",
    "    assert len(dim) == 2\n",
    "    c_rows = dim[0]\n",
    "    c_cols = dim[1]\n",
    "    assert isinstance(z, np.ndarray)\n",
    "    dim = z.shape\n",
    "    assert len(dim) == 2\n",
    "    z_rows = dim[0]\n",
    "    z_cols = dim[1]\n",
    "    assert beta2_rows == beta1_rows == c_rows\n",
    "    assert z_rows == z_cols == c_cols\n",
    "\n",
    "    t_0 = (c.T).dot(beta1)\n",
    "    t_1 = (beta1).dot((c).dot(t_0))\n",
    "    t_2 = (1 / t_1)\n",
    "    t_3 = (beta1).dot(c)\n",
    "    t_4 = (1 / (t_1 ** 2))\n",
    "    t_5 = (beta2).dot((c).dot((z).dot(t_0)))\n",
    "    T_6 = np.outer(beta1, t_3)\n",
    "    functionValue = (t_5 / t_1)\n",
    "    gradient = (((t_2 * np.outer(beta2, (t_3).dot(z.T))) + (t_2 * np.outer(beta1, ((beta2).dot(c)).dot(z)))) - (((t_4 * (beta1).dot((c).dot((z.T).dot((c.T).dot(beta2))))) * T_6) + ((t_4 * t_5) * T_6)))\n",
    "\n",
    "    return functionValue, gradient\n",
    "\n",
    "def checkGradient(beta1, beta2, c, z):\n",
    "    # numerical gradient checking\n",
    "    # f(x + t * delta) - f(x - t * delta) / (2t)\n",
    "    # should be roughly equal to inner product <g, delta>\n",
    "    t = 1E-6\n",
    "    delta = np.random.randn(6, 3)\n",
    "    f1, _ = fAndG(beta1, beta2, c + t * delta, z)\n",
    "    f2, _ = fAndG(beta1, beta2, c - t * delta, z)\n",
    "    f, g = fAndG(beta1, beta2, c, z)\n",
    "    print('approximation error',\n",
    "          np.linalg.norm((f1 - f2) / (2*t) - np.tensordot(g, delta, axes=2)))\n",
    "\n",
    "def generateRandomData():\n",
    "    beta1 = np.random.randn(6)\n",
    "    beta2 = np.random.randn(6)\n",
    "    c = np.random.randn(6, 3)\n",
    "    z = np.random.randn(3, 3)\n",
    "\n",
    "    return beta1, beta2, c, z\n",
    "\n",
    "\n",
    "beta1, beta2, c, z = generateRandomData()\n",
    "functionValue, gradient = fAndG(beta1, beta2, c, z)\n",
    "print('functionValue = ', functionValue)\n",
    "print('gradient = ', gradient)\n",
    "\n",
    "print('numerical gradient checking ...')\n",
    "checkGradient(beta1, beta2, c, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Angular Acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\frac{\\partial \\alpha}{\\partial c_i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sample code automatically generated on 2023-10-31 13:08:52\n",
    "\n",
    "by www.matrixcalculus.org\n",
    "\n",
    "from input\n",
    "\n",
    "d/dc (beta3'*c*z*c'*beta1*beta1'*c*c'*beta1-2*beta2'*c*z*c'*beta1*beta2'*c*c'*beta1)/(beta1'*c*c'*beta1*beta1'*c*c'*beta1) = 1/(beta1'*c*c'*beta1).^2*beta1'*c*c'*beta1*beta3*(beta1'*c*z')+1/(beta1'*c*c'*beta1).^2*beta1'*c*c'*beta1*beta1*(beta3'*c*z)+1/(beta1'*c*c'*beta1).^2*beta1'*c*z'*c'*beta3*beta1*(beta1'*c)+1/(beta1'*c*c'*beta1).^2*beta3'*c*z*c'*beta1*beta1*(beta1'*c)-(2/(beta1'*c*c'*beta1).^2*beta1'*c*c'*beta2*beta2*(beta1'*c*z')+2/(beta1'*c*c'*beta1).^2*beta2'*c*c'*beta1*beta1*(beta2'*c*z)+2/(beta1'*c*c'*beta1).^2*beta1'*c*z'*c'*beta2*beta2*(beta1'*c)+2/(beta1'*c*c'*beta1).^2*beta2'*c*z*c'*beta1*beta1*(beta2'*c))-((2*(beta1'*c*z'*c'*beta3*beta1'*c*c'*beta1-2*beta1'*c*z'*c'*beta2*beta1'*c*c'*beta2))/(beta1'*c*c'*beta1).^4*beta1'*c*c'*beta1*beta1*(beta1'*c)+(2*(beta3'*c*z*c'*beta1*beta1'*c*c'*beta1-2*beta2'*c*z*c'*beta1*beta2'*c*c'*beta1))/(beta1'*c*c'*beta1).^4*beta1'*c*c'*beta1*beta1*(beta1'*c))\n",
    "\n",
    "where\n",
    "\n",
    "beta1 is a vector\n",
    "beta2 is a vector\n",
    "beta3 is a vector\n",
    "c is a matrix\n",
    "z is a matrix\n",
    "\n",
    "The generated code is provided \"as is\" without warranty of any kind.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def fAndG(beta1, beta2, beta3, c, z):\n",
    "    assert isinstance(beta1, np.ndarray)\n",
    "    dim = beta1.shape\n",
    "    assert len(dim) == 1\n",
    "    beta1_rows = dim[0]\n",
    "    assert isinstance(beta2, np.ndarray)\n",
    "    dim = beta2.shape\n",
    "    assert len(dim) == 1\n",
    "    beta2_rows = dim[0]\n",
    "    assert isinstance(beta3, np.ndarray)\n",
    "    dim = beta3.shape\n",
    "    assert len(dim) == 1\n",
    "    beta3_rows = dim[0]\n",
    "    assert isinstance(c, np.ndarray)\n",
    "    dim = c.shape\n",
    "    assert len(dim) == 2\n",
    "    c_rows = dim[0]\n",
    "    c_cols = dim[1]\n",
    "    assert isinstance(z, np.ndarray)\n",
    "    dim = z.shape\n",
    "    assert len(dim) == 2\n",
    "    z_rows = dim[0]\n",
    "    z_cols = dim[1]\n",
    "    assert beta1_rows == beta2_rows == beta3_rows == c_rows\n",
    "    assert z_rows == z_cols == c_cols\n",
    "\n",
    "    t_0 = (c.T).dot(beta1)\n",
    "    t_1 = (c).dot((z).dot(t_0))\n",
    "    t_2 = (c).dot(t_0)\n",
    "    t_3 = (beta1).dot(t_2)\n",
    "    t_4 = (t_3 ** 2)\n",
    "    t_5 = (1 / t_4)\n",
    "    t_6 = (t_5 * t_3)\n",
    "    t_7 = (beta1).dot(c)\n",
    "    t_8 = (beta3).dot(t_1)\n",
    "    T_9 = np.outer(beta1, t_7)\n",
    "    t_10 = (t_7).dot(z.T)\n",
    "    t_11 = (2 / t_4)\n",
    "    t_12 = (beta2).dot(t_2)\n",
    "    t_13 = (c.T).dot(beta2)\n",
    "    t_14 = (beta2).dot(t_1)\n",
    "    t_15 = (beta2).dot(c)\n",
    "    t_16 = (beta1).dot((c).dot((z.T).dot((c.T).dot(beta3))))\n",
    "    t_17 = (beta1).dot((c).dot((z.T).dot(t_13)))\n",
    "    t_18 = (beta1).dot((c).dot(t_13))\n",
    "    t_19 = ((t_8 * t_3) - ((2 * t_14) * t_12))\n",
    "    t_20 = (t_3 ** 4)\n",
    "    functionValue = (t_19 / (t_3 * t_3))\n",
    "    gradient = ((((((t_6 * np.outer(beta3, t_10)) + (t_6 * np.outer(beta1, ((beta3).dot(c)).dot(z)))) + ((t_5 * t_16) * T_9)) + ((t_5 * t_8) * T_9)) - (((((t_11 * t_18) * np.outer(beta2, t_10)) + ((t_11 * t_12) * np.outer(beta1, (t_15).dot(z)))) + ((t_11 * t_17) * np.outer(beta2, t_7))) + ((t_11 * t_14) * np.outer(beta1, t_15)))) - (((((2 * ((t_16 * t_3) - ((2 * t_17) * t_18))) / t_20) * t_3) * T_9) + ((((2 * t_19) / t_20) * t_3) * T_9)))\n",
    "\n",
    "    return functionValue, gradient\n",
    "\n",
    "def checkGradient(beta1, beta2, beta3, c, z):\n",
    "    # numerical gradient checking\n",
    "    # f(x + t * delta) - f(x - t * delta) / (2t)\n",
    "    # should be roughly equal to inner product <g, delta>\n",
    "    t = 1E-6\n",
    "    delta = np.random.randn(3, 3)\n",
    "    f1, _ = fAndG(beta1, beta2, beta3, c + t * delta, z)\n",
    "    f2, _ = fAndG(beta1, beta2, beta3, c - t * delta, z)\n",
    "    f, g = fAndG(beta1, beta2, beta3, c, z)\n",
    "    print('approximation error',\n",
    "          np.linalg.norm((f1 - f2) / (2*t) - np.tensordot(g, delta, axes=2)))\n",
    "\n",
    "def generateRandomData():\n",
    "    beta1 = np.random.randn(3)\n",
    "    beta2 = np.random.randn(3)\n",
    "    beta3 = np.random.randn(3)\n",
    "    c = np.random.randn(3, 3)\n",
    "    z = np.random.randn(3, 3)\n",
    "\n",
    "    return beta1, beta2, beta3, c, z\n",
    "\n",
    "beta1, beta2, beta3, c, z = generateRandomData()\n",
    "functionValue, gradient = fAndG(beta1, beta2, beta3, c, z)\n",
    "print('functionValue = ', functionValue)\n",
    "print('gradient = ', gradient)\n",
    "\n",
    "print('numerical gradient checking ...')\n",
    "checkGradient(beta1, beta2, beta3, c, z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dhma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
